{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo1J9dM3auiE",
        "outputId": "9cdb80ad-0dc9-49d0-fe11-bebe13223882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40 kB 3.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 61 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 71 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 102 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 112 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 122 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 133 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 143 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 153 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 163 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 174 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 194 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 204 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 215 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 225 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 235 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 245 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 256 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 266 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 286 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 296 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 307 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 317 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 327 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 337 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 348 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 358 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 368 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 378 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 389 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 399 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 409 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 419 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 430 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 440 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 450 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 460 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 471 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 481 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 491 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 501 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 512 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 522 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 532 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 542 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 552 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 563 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 573 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 583 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 593 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 604 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 614 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 624 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 634 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 645 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 655 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 665 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 675 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 686 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 696 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 706 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 716 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 727 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 734 kB 3.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 18.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 95 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 39.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U gym\n",
        "%pip install -q -U gym[box2d,atari,accept-rom-license]\n",
        "!apt update &> /dev/null && apt install -y xvfb &> /dev/null\n",
        "%pip install -q -U pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvirtualdisplay as pvd\n",
        "display = pvd.Display(visible=0, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "6FoEhCfLa3ME"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeR_ZCaXblub",
        "outputId": "381ec735-3c5c-41c8-f868-66b30100f5f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset(seed=42)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90rhGvetbqU8",
        "outputId": "79786690-5358-49dc-ccd6-86424184714a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render(mode=\"rgb_array\")\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sUMpyGtbxOG",
        "outputId": "77e89c08-ebd9-4fce-9db5-59abf1b0f2ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 600, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ff17NqTb5Fz",
        "outputId": "d1fdb128-76f9-48ea-96df-3c00273789c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = 1\n",
        "obs, reward, done, info = env.step(action)\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3rqHsu8b7iV",
        "outputId": "1debd6cc-9141-48cf-f6c7-1af9ae13445e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvpBWO-AcCgd",
        "outputId": "e8875abd-fac4-43dd-a2f2-9f76010d10aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AD1iHuXcD4w",
        "outputId": "4ba3f640-3594-4af4-da2f-5c45ef584a46"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRVISPuxcE2q",
        "outputId": "3d0f4ac2-880b-4349-8e2c-da50ba3f678c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_policy(obs):\n",
        "  angle = obs[2]\n",
        "  return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "  episode_rewards = 0\n",
        "  obs = env.reset(seed=episode)\n",
        "  for step in range(200):\n",
        "    action = basic_policy(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    episode_rewards += reward\n",
        "    if done:\n",
        "      break\n",
        "  totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "y_8UvJ-XcFSF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.mean(totals), np.std(totals), min(totals), max(totals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc2JTK2jcky7",
        "outputId": "a389db40-c723-4d17-c484-20583fe5c264"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41.698, 8.389445512070509, 24.0, 63.0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "lXeZJGqocsR1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "  with tf.GradientTape() as tape:\n",
        "    left_proba = model(obs[np.newaxis])\n",
        "    action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "    y_target = tf.constant([1.]) - tf.cast(action, tf.float32)\n",
        "    loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "  \n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  obs, reward, done, info = env.step(int(action))\n",
        "  return obs, reward, info, grads"
      ],
      "metadata": {
        "id": "rIxGldppc6SG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "  all_rewards = []\n",
        "  all_grads = []\n",
        "  for episode in range(n_episodes):\n",
        "    current_rewards = []\n",
        "    current_grads = []\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "      obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "      current_rewards.append(reward)\n",
        "      current_grads.append(grads)\n",
        "      if done:\n",
        "        break\n",
        "    all_rewards.append(current_rewards)\n",
        "    all_grads.append(current_grads)\n",
        "  return all_rewards, all_grads"
      ],
      "metadata": {
        "id": "KYJJdbr_dpTh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "metadata": {
        "id": "R4bTA0k6eXxo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discount_rewards([10, 0, -50], discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9qDuMG3eags",
        "outputId": "ba80410d-1b72-493c-b39b-57a999206056"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-22, -40, -50])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discount_and_normalize_rewards([[10,0,-50], [10,20]], discount_factor=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHGGBtnoehM8",
        "outputId": "f8f4c69f-a01b-486c-eabc-c26c175fcb22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
              " array([1.26665318, 1.0727777 ])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95"
      ],
      "metadata": {
        "id": "rWnAdqkdeoA2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ],
      "metadata": {
        "id": "mKJm2ZI1e41H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(n_iterations):\n",
        "  all_rewards, all_grads = play_multiple_episodes(\n",
        "      env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "  all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
        "\n",
        "  all_mean_grads = []\n",
        "  for var_index in range(len(model.trainable_variables)):\n",
        "    mean_grads = tf.reduce_mean(\n",
        "        [final_reward * all_grads[episode_index][step][var_index]\n",
        "         for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "            for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "    all_mean_grads.append(mean_grads)\n",
        "  optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "lqfLuwtVfC7l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = [  # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "rewards = [  # shape=[s, a, s']\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
        "]\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]"
      ],
      "metadata": {
        "id": "AhAgDQSmgFzH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "  Q_values[state, actions] = 0.0"
      ],
      "metadata": {
        "id": "YPCgXL8Jio5V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.90\n",
        "\n",
        "for iteration in range(50):\n",
        "  Q_prev = Q_values.copy()\n",
        "  for s in range(3):\n",
        "    for a in possible_actions[s]:\n",
        "      Q_values[s, a] = np.sum([\n",
        "          transition_probabilities[s][a][sp]\n",
        "          * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
        "          for sp in range(3)])"
      ],
      "metadata": {
        "id": "jibx3-HBi2Z8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsh0PTyPjR3Z",
        "outputId": "fe5502a5-85a7-44e7-fd83-16341b684a9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18.91891892, 17.02702702, 13.62162162],\n",
              "       [ 0.        ,        -inf, -4.87971488],\n",
              "       [       -inf, 50.13365013,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35R_2uqzjTX2",
        "outputId": "f61a2d86-57d0-4201-979b-13c85ee11313"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "  probas = transition_probabilities[state][action]\n",
        "  next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "  reward = rewards[state][action][next_state]\n",
        "  return next_state, reward"
      ],
      "metadata": {
        "id": "V85x1SbwjWfV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exploitation_policy(state):\n",
        "  return np.random.choice(possible_actions[state])"
      ],
      "metadata": {
        "id": "cgdOnDAKjsHw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha0 = 0.05\n",
        "decay = 0.005\n",
        "gamma = 0.90\n",
        "state = 0\n",
        "\n",
        "for iteration in range(10_000):\n",
        "  action = exploitation_policy(state)\n",
        "  next_state, reward = step(state, action)\n",
        "  next_value = Q_values[next_state].max()\n",
        "  alpha = alpha0 / (1 + iteration * decay)\n",
        "  Q_values[state, action] *= 1 - alpha\n",
        "  Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "  state = next_state"
      ],
      "metadata": {
        "id": "FAV_1lHAjy0P"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = [4]  # == env.observation_space.shape\n",
        "n_outputs = 2  # == env.action_space.n\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ],
      "metadata": {
        "id": "u3bRqcc4kWS_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)  # random action\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])[0]\n",
        "        return Q_values.argmax()  # optimal action according to the DQN"
      ],
      "metadata": {
        "id": "TQiWQyL3kaOz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)"
      ],
      "metadata": {
        "id": "BGMU6nefkcE9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)\n",
        "    ]\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "metadata": {
        "id": "zvXzlcsukeAp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "I-FlmdhvkgJ1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - dones) * discount_factor * max_next_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "nQRG23w4kiiZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)"
      ],
      "metadata": {
        "id": "ClUWpJCDklYm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(obs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLFAmuyLkoGf",
        "outputId": "5fdbbb76-8cd9-4d9b-ec29-4491ccfa1ad1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.429335   2.3622649  0.20011337 0.25885397]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U-WC9OEUtdbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}